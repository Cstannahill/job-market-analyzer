{
  "id": "meta-llama/llama-4-scout:free",
  "canonical_slug": "meta-llama/llama-4-scout-17b-16e-instruct",
  "hugging_face_id": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
  "name": "Meta: Llama 4 Scout (free)",
  "created": 1743881519,
  "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.",
  "context_length": 128000,
  "architecture": {
    "modality": "text+image->text",
    "input_modalities": ["text", "image"],
    "output_modalities": ["text"],
    "tokenizer": "Llama4",
    "instruct_type": null
  },
  "pricing": {
    "prompt": "0",
    "completion": "0",
    "request": "0",
    "image": "0",
    "web_search": "0",
    "internal_reasoning": "0"
  },
  "top_provider": {
    "context_length": 128000,
    "max_completion_tokens": 4028,
    "is_moderated": true
  },
  "per_request_limits": null,
  "supported_parameters": [
    "max_tokens",
    "repetition_penalty",
    "response_format",
    "structured_outputs",
    "temperature",
    "tool_choice",
    "tools",
    "top_k",
    "top_p"
  ],
  "default_parameters": {}
}
